<!DOCTYPE html>
<html>
<head>
<title>The MSD Book :: Quality Attributes</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<p>
	<img src="msd-book-front.png" 
    alt="The MSD Book" width="200" style="float: left; margin: 0 1rem 1rem 0;"/>
	<strong>Quality Attributes (QA)</strong> is one of the highest-leverage topics in architecture. I’ll break it into (1) what quality attributes (QAs) <em>are</em>, (2) a practical process to <em>identify &amp; prioritize</em> them, (3) how they map to tactics/patterns, (4) a real-world cautionary example, (5) concrete <strong>production metrics</strong> to track per QA (with example SLOs/alerts), and (6) a short architecture / code-review checklist and common pitfalls.</p>
<h1 id="1-quick-primer--what-a-qa-is">1) Quick primer — what a QA is</h1>
<p>A <strong>Quality Attribute</strong> (QA) is a non-functional property the system must exhibit: e.g., performance, availability, security, scalability, maintainability, testability, portability, usability, etc. Unlike features (what the system does), QAs describe <em>how</em> it behaves under conditions and over time.</p>
<p>QAs are <em>first-class architecture drivers</em> — they guide major design choices: protocols, component boundaries, data stores, deployment model, resilience tactics, and team/ops practices.</p>
<h1 id="2-practical-process-to-identify--prioritize-qas">2) Practical process to identify &amp; prioritize QAs</h1>
<p>Use a lightweight, repeatable workflow that fits engineering teams:</p>
<ol>
<li>
<p><strong>Stakeholder inventory</strong>
List stakeholders (end users, ops, security, product, legal, support, sales, partners). Ask: what does success look like to each?</p>
</li>
<li>
<p><strong>Capture scenarios (not vague goals)</strong>
For each candidate QA, write concrete scenarios: who, stimulus, environment, and response measure.
Example: “User checkout — under 90th percentile load of 1k RPS, 95% of checkouts must finish within 1.5s.”</p>
</li>
<li>
<p><strong>Utility tree / Scorecard</strong>
Build a utility tree: root = business goal, branches = QAs, leaves = scenarios. For each leaf give: importance (1–10), risk (probable/unknown), and cost to achieve (low/med/high).</p>
</li>
<li>
<p><strong>Prioritization (value × risk)</strong>
Rank scenarios by business value × risk (where risk is likelihood+impact). Prioritize high-value/high-risk attributes first.</p>
</li>
<li>
<p><strong>Translate to tactics &amp; measures</strong>
For the top scenarios choose tactics (caching, sharding, encryption, circuit breaker, canary deploys) and define concrete SLOs/SLA/metrics.</p>
</li>
<li>
<p><strong>Allocate budget &amp; policy</strong>
Decide acceptable tradeoffs — e.g., “we choose higher latency for search during ingestion windows to preserve write throughput.”</p>
</li>
<li>
<p><strong>Iterate</strong>
Revisit after major product changes or incidents.</p>
</li>
</ol>
<h1 id="3-common-qas-%E2%86%92-concrete-tactics--patterns">3) Common QAs → concrete tactics / patterns</h1>
<p>Below are common QAs and the usual tactics and patterns you’ll choose from:</p>
<ul>
<li><strong>Performance / Latency</strong>: caching, CDN, async processing, pre-computation, connection pooling, fast serializers.</li>
<li><strong>Throughput / Scalability</strong>: partitioning/sharding, stateless services, autoscaling, message queues, backpressure.</li>
<li><strong>Availability / Reliability</strong>: redundancy, failover, circuit breakers, bulkheads, retries with backoff, active-active, graceful degradation.</li>
<li><strong>Security</strong>: defense-in-depth, least privilege, encryption in transit &amp; at rest, input validation, WAF, regular pentests.</li>
<li><strong>Maintainability / Modifiability</strong>: clean bounded contexts, small classes/modules, dependency injection, good interfaces, tests, documentation.</li>
<li><strong>Observability</strong>: structured logs, distributed tracing, metrics, dashboards, SLOs/SLA, alerting.</li>
<li><strong>Testability</strong>: dependency inversion, seamable code, contract tests, deterministic CI, environment parity.</li>
<li><strong>Deployability</strong>: CI/CD, feature flags, canary and blue-green deploys, DB migration strategies.</li>
<li><strong>Cost-efficiency</strong>: right-sizing, spot instances, batching, TTLs on caches and logs, lifecycle policies.</li>
</ul>
<h1 id="4-real-world-example-what-happens-when-qas-are-ignored">4) Real-world example: what happens when QAs are ignored</h1>
<p><strong>Example: Knight Capital Group (Aug 2012)</strong> — they deployed new trading software that activated a legacy code path, causing rapid, erroneous trades. The firm lost $440M in 45 minutes and needed external rescue.
Why this fits QA failure:</p>
<ul>
<li><strong>Reliability / Deployability</strong>: inadequate deployment gating and lack of safe rollbacks.</li>
<li><strong>Testing / Observability</strong>: insufficient staging and tests for the new code path.</li>
<li><strong>Change control</strong>: poor deployment automation and feature isolation.</li>
</ul>
<p>Lesson: ignoring deployability and testability (non-functional attributes) can destroy business value in a single release.</p>
<p>(Other famous failures — security breaches like Equifax — show the same: ignoring security QAs and patching practices causes massive damage.)</p>
<h1 id="5-production-metrics-to-track-by-qa-what-to-measure--example-slosalerts">5) Production metrics to track by QA (what to measure + example SLOs/alerts)</h1>
<p>Below are actionable metrics you should instrument and track. Pick the small set that tie directly to your prioritized scenarios.</p>
<hr>
<h2 id="performance-latency">Performance (latency)</h2>
<p><strong>Metrics</strong></p>
<ul>
<li>p50, p95, p99 response latency per endpoint</li>
<li>Request throughput (RPS)</li>
<li>Backend/DB query latency
<strong>SLO examples</strong></li>
<li>95% of <code>POST /checkout</code> requests complete &lt; 1.5s over a 30-day window.
<strong>Alerts</strong></li>
<li>p95 &gt; SLO for X minutes; sudden jump in p99; correlation with CPU/GC spikes.</li>
</ul>
<hr>
<h2 id="availability--reliability">Availability &amp; Reliability</h2>
<p><strong>Metrics</strong></p>
<ul>
<li>Uptime (%), successful requests / total requests, error rate (5xx), healthcheck pass rate</li>
<li>MTTR (mean time to recovery), MTTF</li>
<li>Error budget consumption
<strong>SLO examples</strong></li>
<li>Availability 99.95% monthly for critical API.
<strong>Alerts</strong></li>
<li>Error rate &gt; 1% for 5m; availability falling below error budget.</li>
</ul>
<hr>
<h2 id="scalability">Scalability</h2>
<p><strong>Metrics</strong></p>
<ul>
<li>Max sustainable RPS at acceptable latency, autoscaler cooldowns, queue depths</li>
<li>CPU &amp; memory per node, horizontal scaling events
<strong>SLOs</strong></li>
<li>System handles 2× baseline traffic for 15 minutes with p95 latency under threshold.
<strong>Alerts</strong></li>
<li>Queue depth growing &gt; threshold; new nodes failing to come up.</li>
</ul>
<hr>
<h2 id="security">Security</h2>
<p><strong>Metrics</strong></p>
<ul>
<li>Number of critical vulnerabilities open, time-to-patch, failed login attempts, anomalous traffic, crypto coverage (TLS active)</li>
<li>Rate of privilege-escalation attempts
<strong>SLOs</strong></li>
<li>Critical CVEs remediated within 72 hours.
<strong>Alerts</strong></li>
<li>Sudden spike in failed auth; known exploit detected in dependency.</li>
</ul>
<hr>
<h2 id="maintainability--modifiability">Maintainability / Modifiability</h2>
<p><strong>Metrics</strong></p>
<ul>
<li>Mean time to implement change, cyclomatic complexity per module, code churn, PR review time, test coverage (targeted), number of hotspots (files with frequent bugs)
<strong>SLOs</strong></li>
<li>Average lead time for changes &lt; 3 days for small features; test pass rate 100% in CI.
<strong>Alerts</strong></li>
<li>A hotspot file exceeds complexity threshold or weekly bug rate increases.</li>
</ul>
<hr>
<h2 id="observability">Observability</h2>
<p><strong>Metrics</strong></p>
<ul>
<li>Coverage: percentage of requests traced, ratio of structured logs to requests, metrics per service</li>
<li>Alert noise: false positive alert rate
<strong>SLOs</strong></li>
<li>At least 90% of user-facing requests produce a trace and a correlated request-id in logs.
<strong>Alerts</strong></li>
<li>Trace sampling drops, missing telemetry after deploy.</li>
</ul>
<hr>
<h2 id="testability--quality">Testability / Quality</h2>
<p><strong>Metrics</strong></p>
<ul>
<li>Test flakiness rate, CI pass rate, integration test run time, contract test failures
<strong>SLOs</strong></li>
<li>Nightly full test suite must pass; flakiness &lt; 0.5%
<strong>Alerts</strong></li>
<li>CI failure rate increases or flake count increases.</li>
</ul>
<hr>
<h2 id="deployability--change-failure">Deployability / Change failure</h2>
<p><strong>Metrics</strong></p>
<ul>
<li>Deployment frequency, change failure rate (percentage of deploys causing rollback), time to rollback
<strong>SLOs</strong></li>
<li>Deployment frequency: daily; change failure rate &lt; 3%
<strong>Alerts</strong></li>
<li>Elevated rollback rate, failed canary metrics.</li>
</ul>
<hr>
<h2 id="resilience--survivability">Resilience / Survivability</h2>
<p><strong>Metrics</strong></p>
<ul>
<li>Chaos test pass rate, fallback invocation rate, graceful degradation activations
<strong>SLOs</strong></li>
<li>Circuit breakers open rate below threshold; bulkheads prevent cross-service failure.
<strong>Alerts</strong></li>
<li>Fallbacks triggered &gt; expected baseline.</li>
</ul>
<hr>
<h2 id="cost-efficiency">Cost Efficiency</h2>
<p><strong>Metrics</strong></p>
<ul>
<li>Cost per request, instance utilization, storage costs by tier
<strong>SLOs</strong></li>
<li>Cost per active user &lt; target.
<strong>Alerts</strong></li>
<li>Sudden spike in cloud spend or storage egress.</li>
</ul>
<hr>
<h3 id="how-to-measure--tools">How to measure &amp; tools</h3>
<ul>
<li>Use metrics systems (Prometheus/Grafana, Datadog, New Relic), distributed tracing (OpenTelemetry/Jaeger), structured logs (ELK/Opensearch), and security scanners (Snyk/Dependabot) integrated into CI/CD.</li>
<li>Define SLOs and use an error-budget policy to make release decisions.</li>
</ul>
<h1 id="6-translate-qa-priorities-into-architecture-choices-quick-mapping">6) Translate QA priorities into architecture choices (quick mapping)</h1>
<p>If your top QA is:</p>
<ul>
<li><strong>Latency / performance:</strong> prefer in-memory caches, read replicas, CDNs, co-locate services with databases, async worker queues.</li>
<li><strong>Availability:</strong> multi-AZ/multi-region, redundant services, active-active, circuit breakers, graceful degradation.</li>
<li><strong>Security:</strong> zero-trust, encryption, hardened images, pentest &amp; IR playbook, least privilege.</li>
<li><strong>Maintainability:</strong> smaller services/modules, interfaces, higher test coverage, stricter code review rules.</li>
<li><strong>Throughput/scale:</strong> stateless services, partitioning, event-driven architecture.</li>
<li><strong>Deployability:</strong> robust CI/CD, feature flags, automated DB migrations, canary rollouts.</li>
</ul>
<h1 id="7-example-utility-tree-mini">7) Example utility-tree (mini)</h1>
<pre class="hljs"><code><div>Business goal: Enable global e-commerce with low cart abandonment
  - Performance
     - checkout latency: importance 10; SLO 95% &lt; 1.5s
     - search latency: importance 8; SLO 95% &lt; 300ms
  - Availability
     - checkout availability: importance 10; SLO 99.95%
     - inventory sync: importance 7; SLO 99.9%
  - Security
     - PCI compliance: importance 10; remove cleartext card data
  - Cost
     - infra cost per transaction: importance 4
</div></code></pre>
<h1 id="8-architecture--code-review-qa-checklist-short">8) Architecture / Code-review QA checklist (short)</h1>
<ul>
<li>Did we capture top 3 QAs with scenarios and SLOs? ✅</li>
<li>For each QA: is there at least one measurable metric and an alert? ✅</li>
<li>Are there defensive tactics for failures (circuit breakers, timeouts, bulkheads)? ✅</li>
<li>Are sensitive operations and data protected (encryption, authz)? ✅</li>
<li>Is observability sufficient to debug the scenario (traces + logs + metrics)? ✅</li>
<li>Deploy plan: can we rollback, canary, or dark-launch? ✅</li>
<li>Does the design introduce single points of failure? ✅</li>
<li>Are expected failure modes documented and tested (chaos tests)? ✅</li>
</ul>
<h1 id="9-common-pitfalls-to-avoid">9) Common pitfalls to avoid</h1>
<ul>
<li><strong>Vague QAs</strong>: “we need to be fast” — not helpful. Make scenarios and numbers.</li>
<li><strong>Treat QAs as ops-only</strong>: security, reliability etc. must be designed-in by devs.</li>
<li><strong>Too many QAs</strong>: focus on the few that change architecture; everything cannot be highest priority.</li>
<li><strong>No observability</strong>: you can’t measure or iterate QAs without telemetry.</li>
<li><strong>Ignoring cost</strong>: chasing perfect availability or latency without considering cost often fails in production.</li>
<li><strong>Single-thing optimization</strong>: optimizing one QA without checking regressions in others (e.g., caching to improve latency but bypassing authorization checks).</li>
</ul>
<h1 id="10-quick-playbook-you-can-follow-right-now">10) Quick playbook you can follow right now</h1>
<ol>
<li>Run a <strong>30–60 minute QA workshop</strong>: stakeholders + architects, produce a one-page utility tree with top 3 QAs and 3 scenarios each.</li>
<li>For each top scenario, define SLO and one measurement (metric + threshold).</li>
<li>Map tactics to scenarios and list required infra changes (e.g., add Redis cache, enable distributed tracing, introduce circuit breaker).</li>
<li>Implement telemetry &amp; alerts before doing high-risk change.</li>
<li>Run a small blast radius test (canary or chaos) to validate assumptions.</li>
<li>Revisit after incidents or quarterly.</li>
</ol>
<hr>
<p><strong>NOTE:</strong> This page is generated with assistance from AI tools.</p>

</body>
</html>
